var documenterSearchIndex = {"docs":
[{"location":"user_guide/#User-guide","page":"User guide","title":"User guide","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The list of functions on this page is the officially supported differentiation interface in AbstractDifferentiation.","category":"page"},{"location":"user_guide/#Loading-AbstractDifferentiation","page":"User guide","title":"Loading AbstractDifferentiation","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"To load AbstractDifferentiation, it is recommended to use","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"import AbstractDifferentiation as AD","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"With the AD alias you can access names inside of AbstractDifferentiation using AD.<> instead of typing the long name AbstractDifferentiation.","category":"page"},{"location":"user_guide/#AbstractDifferentiation-backends","page":"User guide","title":"AbstractDifferentiation backends","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"To use AbstractDifferentiation, first construct a backend instance ab::AD.AbstractBackend using your favorite differentiation package in Julia that supports AbstractDifferentiation.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"Here's an example:","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"julia> import AbstractDifferentiation as AD, Zygote\n\njulia> backend = AD.ZygoteBackend();\n\njulia> f(x) = log(sum(exp, x));\n\njulia> AD.gradient(backend, f, collect(1:3))\n([0.09003057317038046, 0.2447284710547977, 0.665240955774822],)","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The following backends are temporarily made available by AbstractDifferentiation as soon as their corresponding package is loaded (thanks to weak dependencies on Julia â‰¥ 1.9 and Requires.jl on older Julia versions):","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.ReverseDiffBackend\nAbstractDifferentiation.ReverseRuleConfigBackend\nAbstractDifferentiation.FiniteDifferencesBackend\nAbstractDifferentiation.ZygoteBackend\nAbstractDifferentiation.ForwardDiffBackend\nAbstractDifferentiation.TrackerBackend","category":"page"},{"location":"user_guide/#AbstractDifferentiation.ReverseDiffBackend","page":"User guide","title":"AbstractDifferentiation.ReverseDiffBackend","text":"ReverseDiffBackend\n\nAD backend that uses reverse mode with ReverseDiff.jl.\n\nnote: Note\nTo be able to use this backend, you have to load ReverseDiff.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#AbstractDifferentiation.ReverseRuleConfigBackend","page":"User guide","title":"AbstractDifferentiation.ReverseRuleConfigBackend","text":"ReverseRuleConfigBackend\n\nAD backend that uses reverse mode with any ChainRulesCore.jl-compatible reverse-mode AD package.\n\nConstructed with a RuleConfig object:\n\nbackend = AD.ReverseRuleConfigBackend(rc)\n\nnote: Note\nOn Julia >= 1.9, you have to load ChainRulesCore (possibly implicitly by loading a ChainRules-compatible AD package) to be able to use this backend.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#AbstractDifferentiation.FiniteDifferencesBackend","page":"User guide","title":"AbstractDifferentiation.FiniteDifferencesBackend","text":"FiniteDifferencesBackend{M}\n\nAD backend that uses forward mode with FiniteDifferences.jl.\n\nThe type parameter M is the type of the method used to perform finite differences.\n\nnote: Note\nTo be able to use this backend, you have to load FiniteDifferences.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#AbstractDifferentiation.ZygoteBackend","page":"User guide","title":"AbstractDifferentiation.ZygoteBackend","text":"ZygoteBackend\n\nCreate an AD backend that uses reverse mode with Zygote.jl.\n\nAlternatively, you can perform AD with Zygote using a special ReverseRuleConfigBackend, namely ReverseRuleConfigBackend(Zygote.ZygoteRuleConfig()). Note, however, that the behaviour of this backend is not equivalent to ZygoteBackend() since the former uses a generic implementation of jacobian etc. for ChainRules-compatible AD backends whereas ZygoteBackend uses implementations in Zygote.jl.\n\nnote: Note\nTo be able to use this backend, you have to load Zygote.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#AbstractDifferentiation.ForwardDiffBackend","page":"User guide","title":"AbstractDifferentiation.ForwardDiffBackend","text":"ForwardDiffBackend{CS}\n\nAD backend that uses forward mode with ForwardDiff.jl.\n\nThe type parameter CS denotes the chunk size of the differentiation algorithm.  If it is Nothing, then ForwardiffDiff uses a heuristic to set the chunk size based on the input.\n\nSee also: ForwardDiff.jl: Configuring Chunk Size\n\nnote: Note\nTo be able to use this backend, you have to load ForwardDiff.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#AbstractDifferentiation.TrackerBackend","page":"User guide","title":"AbstractDifferentiation.TrackerBackend","text":"TrackerBackend\n\nAD backend that uses reverse mode with Tracker.jl.\n\nnote: Note\nTo be able to use this backend, you have to load Tracker.\n\n\n\n\n\n","category":"type"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"In the long term, these backend objects (and many more) will be defined within their respective packages to enforce the AbstractDifferentiation interface. This is already the case for:","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"Diffractor.DiffractorForwardBackend() for Diffractor.jl in forward mode","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"For higher order derivatives, you can build higher order backends using AD.HigherOrderBackend.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.HigherOrderBackend","category":"page"},{"location":"user_guide/#AbstractDifferentiation.HigherOrderBackend","page":"User guide","title":"AbstractDifferentiation.HigherOrderBackend","text":"AD.HigherOrderBackend{B}\n\nLet ab_f be a forward-mode automatic differentiation backend and let ab_r be a reverse-mode automatic differentiation backend. To construct a higher order backend for doing forward-over-reverse-mode automatic differentiation, use AD.HigherOrderBackend((ab_f, ab_r)). To construct a higher order backend for doing reverse-over-forward-mode automatic differentiation, use AD.HigherOrderBackend((ab_r, ab_f)).\n\nFields\n\nbackends::B\n\n\n\n\n\n","category":"type"},{"location":"user_guide/#Derivatives","page":"User guide","title":"Derivatives","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The following list of functions can be used to request the derivative, gradient, Jacobian, second derivative or Hessian without the function value.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.derivative\nAbstractDifferentiation.gradient\nAbstractDifferentiation.jacobian\nAbstractDifferentiation.second_derivative\nAbstractDifferentiation.hessian","category":"page"},{"location":"user_guide/#AbstractDifferentiation.derivative","page":"User guide","title":"AbstractDifferentiation.derivative","text":"AD.derivative(ab::AD.AbstractBackend, f, xs::Number...)\n\nCompute the derivatives of f with respect to the numbers xs using the backend ab.\n\nThe function returns a Tuple of derivatives, one for each element in xs.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.gradient","page":"User guide","title":"AbstractDifferentiation.gradient","text":"AD.gradient(ab::AD.AbstractBackend, f, xs...)\n\nCompute the gradients of f with respect to the inputs xs using the backend ab.\n\nThe function returns a Tuple of gradients, one for each element in xs.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.jacobian","page":"User guide","title":"AbstractDifferentiation.jacobian","text":"AD.jacobian(ab::AD.AbstractBackend, f, xs...)\n\nCompute the Jacobians of f with respect to the inputs xs using the backend ab.\n\nThe function returns a Tuple of Jacobians, one for each element in xs.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.second_derivative","page":"User guide","title":"AbstractDifferentiation.second_derivative","text":"AD.second_derivative(ab::AD.AbstractBackend, f, x)\n\nCompute the second derivative of f with respect to the input x using the backend ab.\n\nThe function returns a single value because second_derivative currently only supports a single input.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.hessian","page":"User guide","title":"AbstractDifferentiation.hessian","text":"AD.hessian(ab::AD.AbstractBackend, f, x)\n\nCompute the Hessian of f wrt the input x using the backend ab.\n\nThe function returns a single matrix because hessian currently only supports a single input.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#Value-and-derivatives","page":"User guide","title":"Value and derivatives","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The following list of functions can be used to request the function value along with its derivative, gradient, Jacobian, second derivative, or Hessian. You can also request the function value, its derivative (or its gradient) and its second derivative (or Hessian) for single-input functions.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.value_and_derivative\nAbstractDifferentiation.value_and_gradient\nAbstractDifferentiation.value_and_jacobian\nAbstractDifferentiation.value_and_second_derivative\nAbstractDifferentiation.value_and_hessian\nAbstractDifferentiation.value_derivative_and_second_derivative\nAbstractDifferentiation.value_gradient_and_hessian","category":"page"},{"location":"user_guide/#AbstractDifferentiation.value_and_derivative","page":"User guide","title":"AbstractDifferentiation.value_and_derivative","text":"AD.value_and_derivative(ab::AD.AbstractBackend, f, xs::Number...)\n\nReturn the tuple (v, ds) of the function value v = f(xs...) and the derivatives ds = AD.derivative(ab, f, xs...).\n\nSee also AbstractDifferentiation.derivative.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_gradient","page":"User guide","title":"AbstractDifferentiation.value_and_gradient","text":"AD.value_and_gradient(ab::AD.AbstractBackend, f, xs...)\n\nReturn the tuple (v, gs) of the function value v = f(xs...) and the gradients gs = AD.gradient(ab, f, xs...).\n\nSee also AbstractDifferentiation.gradient.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_jacobian","page":"User guide","title":"AbstractDifferentiation.value_and_jacobian","text":"AD.value_and_jacobian(ab::AD.AbstractBackend, f, xs...)\n\nReturn the tuple (v, Js) of the function value v = f(xs...) and the Jacobians Js = AD.jacobian(ab, f, xs...).\n\nSee also AbstractDifferentiation.jacobian.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_second_derivative","page":"User guide","title":"AbstractDifferentiation.value_and_second_derivative","text":"AD.value_and_second_derivative(ab::AD.AbstractBackend, f, x)\n\nReturn the tuple (v, d2) of the function value v = f(x) and the second derivative d2 = AD.second_derivative(ab, f, x).\n\nSee also AbstractDifferentiation.second_derivative\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_hessian","page":"User guide","title":"AbstractDifferentiation.value_and_hessian","text":"AD.value_and_hessian(ab::AD.AbstractBackend, f, x)\n\nReturn the tuple (v, H) of the function value v = f(x) and the Hessian H = AD.hessian(ab, f, x).\n\nSee also AbstractDifferentiation.hessian. \n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_derivative_and_second_derivative","page":"User guide","title":"AbstractDifferentiation.value_derivative_and_second_derivative","text":"AD.value_derivative_and_second_derivative(ab::AD.AbstractBackend, f, x)\n\nReturn the tuple (v, d, d2) of the function value v = f(x), the first derivative d = AD.derivative(ab, f, x), and the second derivative d2 = AD.second_derivative(ab, f, x).\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_gradient_and_hessian","page":"User guide","title":"AbstractDifferentiation.value_gradient_and_hessian","text":"AD.value_gradient_and_hessian(ab::AD.AbstractBackend, f, x)\n\nReturn the tuple (v, g, H) of the function value v = f(x), the gradient g = AD.gradient(ab, f, x), and the Hessian H = AD.hessian(ab, f, x).\n\nSee also AbstractDifferentiation.gradient and AbstractDifferentiation.hessian.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#Jacobian-vector-products","page":"User guide","title":"Jacobian-vector products","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"This operation goes by a few names, like \"pushforward\". Refer to the ChainRules documentation for more on terminology. For a single input, single output function f with a Jacobian J, the pushforward operator pf_f is equivalent to applying the function v -> J * v on a (tangent) vector v.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The following functions can be used to request a function that returns the pushforward operator/function. In order to request the pushforward function pf_f of a function f at the inputs xs, you can use either of:","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.pushforward_function\nAbstractDifferentiation.value_and_pushforward_function","category":"page"},{"location":"user_guide/#AbstractDifferentiation.pushforward_function","page":"User guide","title":"AbstractDifferentiation.pushforward_function","text":"AD.pushforward_function(ab::AD.AbstractBackend, f, xs...)\n\nReturn the pushforward function pff of the function f at the inputs xs using backend ab. \n\nThe pushfoward function pff accepts as input a Tuple of tangents, one for each element in xs. If xs consists of a single element, pff can also accept a single tangent instead of a 1-tuple.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_pushforward_function","page":"User guide","title":"AbstractDifferentiation.value_and_pushforward_function","text":"AD.value_and_pushforward_function(ab::AD.AbstractBackend, f, xs...)\n\nReturn a single function vpff which, given tangents ts, computes the tuple (v, p) = vpff(ts) composed of\n\nthe function value v = f(xs...)\nthe pushforward value p = pff(ts) given by the pushforward function pff = AD.pushforward_function(ab, f, xs...) applied to ts.\n\nSee also AbstractDifferentiation.pushforward_function.\n\nwarning: Warning\nThis name should be understood as \"(value and pushforward) function\", and thus is not aligned with the reverse mode counterpart AbstractDifferentiation.value_and_pullback_function.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#Vector-Jacobian-products","page":"User guide","title":"Vector-Jacobian products","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"This operation goes by a few names, like \"pullback\". Refer to the ChainRules documentation for more on terminology. For a single input, single output function f with a Jacobian J, the pullback operator pb_f is equivalent to applying the function v -> v' * J on a (co-tangent) vector v.","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"The following functions can be used to request the pullback operator/function with or without the function value. In order to request the pullback function pb_f of a function f at the inputs xs, you can use either of:","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.pullback_function\nAbstractDifferentiation.value_and_pullback_function","category":"page"},{"location":"user_guide/#AbstractDifferentiation.pullback_function","page":"User guide","title":"AbstractDifferentiation.pullback_function","text":"AD.pullback_function(ab::AD.AbstractBackend, f, xs...)\n\nReturn the pullback function pbf of the function f at the inputs xs using backend ab. \n\nThe pullback function pbf accepts as input a Tuple of cotangents, one for each output of f. If f has a single output, pbf can also accept a single input instead of a 1-tuple.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.value_and_pullback_function","page":"User guide","title":"AbstractDifferentiation.value_and_pullback_function","text":"AD.value_and_pullback_function(ab::AD.AbstractBackend, f, xs...)\n\nReturn a tuple (v, pbf) of the function value v = f(xs...) and the pullback function pbf = AD.pullback_function(ab, f, xs...).\n\nSee also AbstractDifferentiation.pullback_function.\n\nwarning: Warning\nThis name should be understood as \"value and (pullback function)\", and thus is not aligned with the forward mode counterpart AbstractDifferentiation.value_and_pushforward_function.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#Lazy-operators","page":"User guide","title":"Lazy operators","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"You can also get a struct for the lazy derivative/gradient/Jacobian/Hessian of a function. You can then use the * operator to apply the lazy operator on a value or tuple of the correct shape. To get a lazy derivative/gradient/Jacobian/Hessian use any one of:","category":"page"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"AbstractDifferentiation.lazy_derivative\nAbstractDifferentiation.lazy_gradient\nAbstractDifferentiation.lazy_jacobian\nAbstractDifferentiation.lazy_hessian","category":"page"},{"location":"user_guide/#AbstractDifferentiation.lazy_derivative","page":"User guide","title":"AbstractDifferentiation.lazy_derivative","text":"AD.lazy_derivative(ab::AbstractBackend, f, xs::Number...)\n\nReturn an operator ld for multiplying by the derivative of f at xs.\n\nYou can apply the operator by multiplication e.g. ld * y where y is a number if f has a single input, a tuple of the same length as xs if f has multiple inputs, or an array of numbers/tuples.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.lazy_gradient","page":"User guide","title":"AbstractDifferentiation.lazy_gradient","text":"AD.lazy_gradient(ab::AbstractBackend, f, xs...)\n\nReturn an operator lg for multiplying by the gradient of f at xs.\n\nYou can apply the operator by multiplication e.g. lg * y where y is a number if f has a single input or a tuple of the same length as xs if f has multiple inputs.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.lazy_jacobian","page":"User guide","title":"AbstractDifferentiation.lazy_jacobian","text":"AD.lazy_jacobian(ab::AbstractBackend, f, xs...)\n\nReturn an operator lj for multiplying by the Jacobian of f at xs.\n\nYou can apply the operator by multiplication e.g. lj * y or y' * lj where y is a number, vector or tuple of numbers and/or vectors.  If f has multiple inputs, y in lj * y should be a tuple. If f has multiple outputs, y in y' * lj should be a tuple. Otherwise, it should be a scalar or a vector of the appropriate length.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#AbstractDifferentiation.lazy_hessian","page":"User guide","title":"AbstractDifferentiation.lazy_hessian","text":"AD.lazy_hessian(ab::AbstractBackend, f, x)\n\nReturn an operator lh for multiplying by the Hessian of the scalar-valued function f at x.\n\nYou can apply the operator by multiplication e.g. lh * y or y' * lh where y is a number or a vector of the appropriate length.\n\n\n\n\n\n","category":"function"},{"location":"user_guide/#Index","page":"User guide","title":"Index","text":"","category":"section"},{"location":"user_guide/","page":"User guide","title":"User guide","text":"","category":"page"},{"location":"implementer_guide/#Implementer-guide","page":"Implementer guide","title":"Implementer guide","text":"","category":"section"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"warning: Work in progress\nCome back later!","category":"page"},{"location":"implementer_guide/#The-macro-@primitive","page":"Implementer guide","title":"The macro @primitive","text":"","category":"section"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"To implement the AbstractDifferentiation interface for your backend, you only need to provide a \"primitive\" from which the rest of the functions can be deduced. However, for performance reasons, you can implement more of the interface to make certain calls faster.","category":"page"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"At the moment, the only primitives supported are AD.pushforward_function and AD.value_and_pullback_function. The AD.@primitive macro uses the provided function to implement AD.jacobian, and all the other functions follow.","category":"page"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"AD.@primitive function AD.myprimitive(ab::MyBackend, f, xs...)\n    # write your code here\nend","category":"page"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"See the backend-specific extensions in the ext/ folder of the repository for example implementations.","category":"page"},{"location":"implementer_guide/#Function-dependency-graph","page":"Implementer guide","title":"Function dependency graph","text":"","category":"section"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"These details are not part of the public API and are expected to change. They are just listed here to help readers figure out the code structure:","category":"page"},{"location":"implementer_guide/","page":"Implementer guide","title":"Implementer guide","text":"jacobian has no default implementation\nderivative calls jacobian\ngradient calls jacobian\nhessian calls jacobian and gradient\nsecond_derivative calls derivative\nvalue_and_jacobian calls jacobian\nvalue_and_derivative calls value_and_jacobian\nvalue_and_gradient calls value_and_jacobian\nvalue_and_hessian calls jacobian and gradient\nvalue_and_second_derivative calls second_derivative\nvalue_gradient_and_hessian calls value_and_jacobian and gradient\nvalue_derivative_and_second_derivative calls value_and_derivative and second_derivative\npushforward_function calls jacobian\nvalue_and_pushforward_function calls pushforward_function\npullback_function calls value_and_pullback_function\nvalue_and_pullback_function calls gradient","category":"page"},{"location":"","page":"Home","title":"Home","text":"EditURL = \"https://github.com/JuliaDiff/AbstractDifferentiation.jl/blob/master/README.md\"","category":"page"},{"location":"#AbstractDifferentiation","page":"Home","title":"AbstractDifferentiation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: CI) (Image: Coverage) (Image: Code Style: Blue) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages)","category":"page"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is a package that implements an abstract interface for differentiation in Julia. This is particularly useful for implementing abstract algorithms requiring derivatives, gradients, jacobians, Hessians or multiple of those without depending on specific automatic differentiation packages' user interfaces.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia has more (automatic) differentiation packages than you can count on 2 hands. Different packages have different user interfaces. Therefore, having a backend-agnostic interface to request the function value and its gradient for example is necessary to avoid a combinatorial explosion of code when trying to support every differentiation package in Julia in every algorithm package requiring gradients. For higher order derivatives, the situation is even more dire since you can combine any 2 differentiation backends together to create a new higher-order backend.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are an autodiff user and want to write code in a backend-agnostic way, read the user guide in the docs.\nIf you are an autodiff developer and want your backend to implement the interface, read the implementer guide in the docs (still in construction).","category":"page"},{"location":"#Citing-this-package","page":"Home","title":"Citing this package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use this package in your work, please cite the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{schafer2021abstractdifferentiation,\n  title={AbstractDifferentiation. jl: Backend-Agnostic Differentiable Programming in Julia},\n  author={Sch{\\\"a}fer, Frank and Tarek, Mohamed and White, Lyndon and Rackauckas, Chris},\n  journal={NeurIPS 2021 Differentiable Programming Workshop},\n  year={2021}\n}","category":"page"}]
}
